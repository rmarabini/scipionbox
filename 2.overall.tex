\section{Overall System Design: Monitors, Consensus and Streaming}
\label{overall}

In the following we describe the main modifications introduced in the original \scipion version in order to facilitate its deployment in centralized facilities. So far, \scipion is running in three of such facilities, and each installation has been tailored to the particular needs of each facility. The original design satisfied the requirements of our home facility (National Center for Biotechnology, Madrid, Spain), in which \scipion automatically fetches newly recorded movies from a network mounted disk. After this first installation, \scipion was deployed on XXXXX (a centralized facility located at Stockholm [**provide full identification @delarosa]) where a rigorous booking system is followed. There, \scipion was adapted to interface with this booking platform in order to produce better reports. Lately,  \scipion has been installed in a large synchrotron  such as Diamond (Oxford, UK) where projects are handled by ISPyB (a customized laboratory management information system -LIMBS- \citep{Delageniere2011}) and data are saved on a distributed file system that heavily penalizes access to lists of files. In this environment, \scipion needs to constantly interchange information with ISPyB, and it cannot longer check if there is a new file in a particular storage device but must wait to be informed by the system that a new file has been created. All these particularities, that will be described in detail later, could only be handled thanks to a careful design that prioritizes flexibility at many levels. 

%\scipionbox uses the infrastructure provided by the Scipion project [Ref if not given already] that takes care of the creation and execution of workflows. These workflows may be imported from a set of predefined ones or created by the final users even if they have no programing skills.

%A typical \scipion workflow starts importing the microscope data, then the  beam-induced movement is corrected. For this task, \scipion allows you to choose between five different algorithms from three different labs (an exhaustive list containing the approximately 200 algorithms accessible through \scipion and ranked by usability is available at \url{http://calm-shelf-73264.herokuapp.com/report_protocols/protocolTable/}). This multiplicity of possibilities is available at each different processing step although, if found overwhelming, may be ignored by choosing one of the default prepackaged workflows.   

Since \scipion original publication we have developed many \emph{metaprotocols}, that is, 
protocols that either check the progress of other protocols or compare the results of equivalent protocols.

The first type of metaprotocols are called \emph{monitors} and are used to produce live analysis plots, generate reports or raise alerts when some problems are detected. A monitor example is the \emph{CTF-monitor}, that checks the computed defocus values for
each micrograph as they are generated. CTF-monitor may raise an alert if the defocus values are above or below certain thresholds and continuously generate HTML files so that users and staff may easily follow the data acquisition and processing in-house or remotely. At the project's end, a review of the processing history is produced. [** URL to report example?]
%This feature was very important since we knew that difference facilities will have different needs and criteria to be fulfilled.

The second set of metaprotocols are grouped under the name \emph{consensus}. For a given logical step (for example, particle picking) these protocols check if the datasets obtained from the same input data using different algorithm are consistent. Continuing with the particle picking example, \emph{consensus picking} will compare the particles selected for each one of the executed particle-picking algorithms and report on the particles selected by most of them.

The main change introduced in \scipion is \emph{stream processing}. Stream processing has been designed to analyze and react on real-time so that enables analysis of data as it is produced.
In this way a new level of parallelization has been reached since the  data generated by the microscope continuously flows through a chain of protocols. For example, as movies are imported to the system they are aligned and their CTF is estimated. In \scipion stable version users may find  the first steps in the image processing adapted to stream. These steps include: movie alignment, CTF estimation and particle picking. In \scipion developers version all several 2D classification algorithms algorithms are available.

\scipion stream implementation is based on periodic updates of the protocol input and output data during the image processing. Since \scipion runs on threads, it is possible to devote one thread to update these information while the rest of the CPU power is devoted to run the different protocols. The use of threads marks an important difference between running a script or using \scipion, since it means that each module is run independently. For example, Do you want to add a new protocol or monitor in your workflow while it is running? No problem, do it. \scipion allows users to modify  workflows on the fly without modifying a single code line. The modification will not interfere with the already executed steps and may take advantage from all already computed data. Did something go wrong and the microscope stopped? By default \scipion will stop after 24 hours with no microscope activity, but if you wish to continue your acquisition after a stop just press the continue button, there is no need to restart from the beginning. 

Last but not least \scipion provides full data provenance. Data provenance is an important form of metadata that describes how a particular data set was generated by detailing: the steps, input data and parameters used in the computation. Data provenance guaranties reproducibility and traceability. %Since provenance is solved by Scipion and not Scipionbox we will not discuss here how it is been achieved. We just note that, before choosing which data has to be stored, it is necessary to define how these data have to be structured so that they can be later recovered and understood . %This abstract modeling of the data also simplifies the software interoperability offered by Scipion.


%Standardization
%User has access to all parameters used by algorithms (she may ignore them)
%Project can be imported in scipion without further ado.
%Gui vs programatico
%Export options?


 

