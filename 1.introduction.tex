
\section{Introduction}

3D electron microscopy (3DEM)  can provide rich information about structural characteristics of macromolecular complexes. The field is under a major transformation due to the arrival of better microscopes, new large area detectors and automatization \citep{kuhlbrandt2014a, Kuhlbrandt2014b}. These improvements make possible that a single microscope generates high quality data sets on the order of terabytes per day \citep{Saibil2015} while working for several days without interruption. Major challenges faced in the field are: (1) an efficient management of these huge datasets and their corresponding image processing workflows; (2) a raising interest from scientists outside the field who might lack the skills of a experienced microscopist and; (3) the need for an effective data and metadata transfer in an increasingly distributed and collaborative environment. %This situation strongly demands the application of techniques that are highly automated to allow high throughput. 

%We will focus in this article in the critical stage of the image data acquisition in 3DEM.  Our goal is  to further develop  (\scipion) such that it extends the functionality of the microscope data collecting programs. In this way, \scipion starts the processing of the movies as they are being acquired with the double aim of checking for possible data collection errors and execute a first user-tailored image processing workflow that provides users with an accurate measurement of the acquired data quality.

%Accounting for major government investment in increasingly costly electron microscopes requires a large user  base. 

Clearly, the future of projects that require high-end microscopes is through centralized microscopy facilities, in which automatic acquired data needs to be monitored by users and facility staff.  \scipion has been redesigned with this scenario in mind. % in which the early detection of issues may save a data acquisition shift which otherwise needs to be repeated after a new application that may be granted many months later. 
In this way, \scipion starts the processing of the movies as they are being acquired with the double aim of checking for possible data collection errors and execute a first user-tailored image processing workflow that provides users with an accurate measurement of the acquired data quality.

Many software packages have been developed to provide automated data collection, such as SerialEM \citep{Mastronarde2005}, Leginon \citep{Suloway2009}, UCSF-Tomography \citep{Zheng2007}, Tom Toolbox \citep{Nickell2005}, EPU \citep{EPU}, Latitude in Digital Micrograph \citep{Latitude}, EMMenu \citep{emmenu}, etc. However, the next step,  which is to provide image processing while  data are being collected, is not so well addressed. In fact, in most places this is accomplished through home made scripts \citep[i.e.][]{Pichkur2018}. %(i.e. \citet{Pichkur2018}).
Exceptions are UCSFImage4 \citep{Li2015} and Focus \citep{Biyani2017}. As we will show in this article, \scipion differs from these solutions in a larger flexibility both from the point of view of users and facility staff.
